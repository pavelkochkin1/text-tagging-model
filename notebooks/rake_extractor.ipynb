{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HquTem1Cdq6e"
      },
      "source": [
        "1. Класс для извлечения ключевых слов\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1AkTSZsei9Z"
      },
      "source": [
        "Извлекаем ключевые слова с помощью библиотеки RAKE, который подходит для выделения основных идей и тем текста.\n",
        "\n",
        "**RAKE (Rapid Automatic Keyword Extraction)** — это алгоритм для извлечения ключевых слов из текста с помощью анализа частот слов и их взаимосвяей. Слова и фразы, которые часто встречаются вместе и редко встречаются в других контекстах текста **считаются ключевыми**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwXsTSliedS9",
        "outputId": "c1225bd5-b15c-472f-b0e1-b96b69697ab8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/pavelkockin/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/pavelkockin/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from rake_nltk import Rake\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "# Предварительная подготовка\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "uGkhrlYfdO3K"
      },
      "outputs": [],
      "source": [
        "class RakeKeyphrasesExtractor:\n",
        "    def __init__(self, language=\"russian\", max_length=10):\n",
        "        self.language = language\n",
        "        self.max_length = max_length\n",
        "        self.stopwords = stopwords.words(self.language)\n",
        "        self.rake = Rake(\n",
        "            stopwords=self.stopwords, language=self.language, max_length=self.max_length\n",
        "        )\n",
        "\n",
        "    def extract(self, text):\n",
        "        self.rake.extract_keywords_from_text(text)\n",
        "        keyphrases = self.rake.get_ranked_phrases_with_scores()\n",
        "        return keyphrases\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdJizEw7dyjQ"
      },
      "source": [
        "2. Класс для нормализации текста\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "mkJ64HWGdxa5"
      },
      "outputs": [],
      "source": [
        "import pymorphy2\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "class TextNormalizer:\n",
        "    def __init__(self, language=\"ru\"):\n",
        "        self.morph = pymorphy2.MorphAnalyzer(lang=language)\n",
        "\n",
        "    def normalize(self, keyphrases):\n",
        "        filtered_tokens = []\n",
        "        for _, text in keyphrases:\n",
        "            for word in text.split():\n",
        "                p = self.morph.parse(str(word))[0]\n",
        "                if p.tag.POS == \"NOUN\":\n",
        "                    filtered_tokens.append(p.normal_form)\n",
        "        return Counter(filtered_tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAMBfIXUd2P-"
      },
      "source": [
        "3. Класс для анализа на основе векторов слов\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "n1HrrVY7d3TJ"
      },
      "outputs": [],
      "source": [
        "import fasttext\n",
        "import fasttext.util\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class WordVectorAnalyzer:\n",
        "    def __init__(self, language=\"ru\", model_name=\"cc.ru.300.bin\"):\n",
        "        fasttext.util.download_model(language, if_exists=\"ignore\")\n",
        "        self.ft_model = fasttext.load_model(model_name)\n",
        "\n",
        "    def analyze(self, words, top_n=5):\n",
        "        wordlist_vec = [self.ft_model[word] for word in words]\n",
        "        distances = pairwise_distances(wordlist_vec, wordlist_vec, \"cosine\")\n",
        "        keywords_with_distance = list(zip(words, distances.mean(axis=1)))\n",
        "        keywords_with_distance = sorted(keywords_with_distance, key=lambda x: -x[1])\n",
        "        return [keyword for keyword, _ in keywords_with_distance][:top_n]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G73KTWyRd5eR"
      },
      "source": [
        "4. Всё запускаем"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "3Sw86KZZd3pn"
      },
      "outputs": [],
      "source": [
        "class KeywordExtractor:\n",
        "    def __init__(self, language):\n",
        "        self.language = language\n",
        "        self.extractor = RakeKeyphrasesExtractor(language=language)\n",
        "        self.normalizer = TextNormalizer(language=\"ru\")\n",
        "        self.analyzer = WordVectorAnalyzer(language=\"ru\")\n",
        "\n",
        "    def extract(self, text: str, top_n: int, min_keyword_cnt: int = 2):\n",
        "        print(\"Экстрактим кейворды...\")\n",
        "        keyphrases = self.extractor.extract(text)\n",
        "        print(\"Нормализуем и фильтруем кейворды...\")\n",
        "        normalized_words = self.normalizer.normalize(keyphrases)\n",
        "        most_common_words = [\n",
        "            word for word, cnt in normalized_words.most_common(top_n) if cnt >= min_keyword_cnt\n",
        "        ]\n",
        "        print(\"Анализируем кейворды для хэштэга...\")\n",
        "        keywords = self.analyzer.analyze(most_common_words, top_n=top_n)\n",
        "        return keywords\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ],
      "source": [
        "class KeywordExtractorApplication:\n",
        "    def __init__(self, language=\"russian\"):\n",
        "        self.language = language\n",
        "        self.extractor = KeywordExtractor(self.language)\n",
        "\n",
        "    def get_keywords(self, text, top_n):\n",
        "        return self.extractor.extract(text, top_n)\n",
        "\n",
        "\n",
        "app = KeywordExtractorApplication()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "icl2dJu5eOIW",
        "outputId": "3b198b04-6ff0-48e0-f170-f7eddd7fc931"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Экстрактим кейворды...\n",
            "Нормализуем и фильтруем кейворды...\n",
            "Анализируем кейворды для хэштэга...\n",
            "['модель', 'задача', 'метод', 'студент', 'учитель']\n"
          ]
        }
      ],
      "source": [
        "source_text = \"\"\"\n",
        "Методы ускорения инференса. \n",
        "Дистилляция\n",
        "Обычно в этом методе есть какая-то большая модель, которую мы называем учитель (teacher), и модель поменьше — студент (student). Хорошим примером будет YandexGPT 3 — большая LLM, способная решать задачу с наилучшим качеством, но она совершенно не укладывается в наш вычислительный бюджет. Есть модель поменьше, вроде Т5, которая потребляет сильно меньше ресурсов, но не решает задачу так же качественно, как YandexGPT 3. Задача Knowledge Distillation состоит в том, чтобы минимизировать потери (loss) между фичами — предсказаниями учителя и студента. \n",
        "\"\"\"\n",
        "\n",
        "hashtags = app.get_keywords(source_text, top_n=10)\n",
        "print(hashtags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
