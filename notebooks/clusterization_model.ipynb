{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyO3tcR3Uf2jWbscc6+D+EKX"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WMo5j7OxvmMH",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1712513317113,
     "user_tz": -180,
     "elapsed": 21527,
     "user": {
      "displayName": "Денис Кажекин",
      "userId": "16634474198373539058"
     }
    },
    "outputId": "fc06881a-5200-4d21-b40f-514adcac2bb2"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m27.0/27.0 MB\u001B[0m \u001B[31m31.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h"
     ]
    }
   ],
   "source": [
    "!pip --quiet install pymystem3 nltk gensim kneed faiss-cpu rouge tqdm"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from gensim.models import KeyedVectors\n",
    "from google.colab import drive\n",
    "from sklearn.cluster import DBSCAN\n",
    "from tqdm import tqdm\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VkkgAd2Ivw30",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1712517317786,
     "user_tz": -180,
     "elapsed": 1742,
     "user": {
      "displayName": "Денис Кажекин",
      "userId": "16634474198373539058"
     }
    },
    "outputId": "7149d259-3801-436b-b32b-71ed17abdf57"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Defining stopwords\n",
    "russian_stopwords = stopwords.words('russian')\n",
    "\n",
    "# Defining the pymystem analyzer\n",
    "nlp = Mystem()\n",
    "\n",
    "# Load the RusVectōrēs model\n",
    "emb_path = '/content/drive/MyDrive/StudCamp HSE x YSDA/model.bin'\n",
    "emb_model = KeyedVectors.load_word2vec_format(emb_path, binary=True)"
   ],
   "metadata": {
    "id": "Fze8bKYrv5FA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class Analyzer:\n",
    "\n",
    "  def __init__(self, analyzer: Mystem, stopwords: list) -> None:\n",
    "    self.analyzer = analyzer\n",
    "    self.stopwords = stopwords\n",
    "\n",
    "  def get_nouns(self, text: str) -> list:\n",
    "    summary = self.analyzer.analyze(text)\n",
    "\n",
    "    nouns = []\n",
    "    for item in summary:\n",
    "      if 'analysis' in item and item['analysis']:\n",
    "        analysis = item['analysis'][0]\n",
    "        if 'gr' in analysis:\n",
    "          pos = analysis['gr'].split('=')[0].split(',')[0]\n",
    "          if pos == 'S' and analysis['lex'] not in self.stopwords:\n",
    "            nouns.append(analysis['lex'])\n",
    "    return nouns"
   ],
   "metadata": {
    "id": "KRZFrwWQyb0B"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class Vectorizer:\n",
    "\n",
    "  def __init__(self, emb_model: KeyedVectors) -> None:\n",
    "    self.emb_model = emb_model\n",
    "\n",
    "  def vectorize_nouns(self, nouns: list) -> np.ndarray:\n",
    "    embeddedNouns = [self.emb_model[noun + '_NOUN'] for noun in nouns if noun + '_NOUN' in self.emb_model]\n",
    "    return np.array(embeddedNouns)"
   ],
   "metadata": {
    "id": "aFewO3Fj0Dxk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class Clusterizer:\n",
    "\n",
    "  def __init__(self) -> None:\n",
    "    pass\n",
    "\n",
    "  def get_clusters_centroids(self, vectorized_nouns: np.ndarray) -> list:\n",
    "\n",
    "    labels, samples = None, None\n",
    "    for parameter in range(2, int(np.sqrt(vectorized_nouns.shape[0]))):\n",
    "      dbscan = DBSCAN(min_samples=parameter, eps=0.1, metric='cosine');\n",
    "      model = dbscan.fit(vectorized_nouns)\n",
    "      labels = model.labels_\n",
    "      samples = parameter\n",
    "\n",
    "      if len(set(labels) - {-1}) < 6:\n",
    "        break\n",
    "\n",
    "    clusters = {}\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "    for cluster_id in range(n_clusters):\n",
    "        points_in_cluster = vectorized_nouns[labels == cluster_id]\n",
    "        clusters[cluster_id] = points_in_cluster\n",
    "\n",
    "    centroids = [np.mean(embeddings, axis=0) for key, embeddings in clusters.items()]\n",
    "    return centroids"
   ],
   "metadata": {
    "id": "jzSjPf_209YQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class FaissKeywordExtractor:\n",
    "\n",
    "  def __init__(self, emb_model: KeyedVectors) -> None:\n",
    "    self.word_vectors = np.array([emb_model[word] for word in emb_model.key_to_index.keys()])\n",
    "    self.words = list(emb_model.key_to_index.keys())\n",
    "    self.index = faiss.IndexFlatL2(self.word_vectors.shape[1])\n",
    "    self.index.add(self.word_vectors.astype(np.float32))\n",
    "\n",
    "  def find_closest(self, vec: np.ndarray) -> str:\n",
    "    query_vector = vec.astype(np.float32)\n",
    "    _, indices = self.index.search(query_vector.reshape(1, -1), 1)\n",
    "    most_similar_word = self.words[indices[0][0]]\n",
    "    return most_similar_word\n",
    "\n",
    "  def get_tags(self, centroids: list) -> list:\n",
    "    tags = [self.find_closest(centroid).split('_')[0] for centroid in centroids]\n",
    "    return tags"
   ],
   "metadata": {
    "id": "5RTzHd0h3U7D"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "text1 = \"\"\" Яндекс открыл набор на студкемпы — бесплатные сверхинтенсивные программы для студентов IT-специальностей. За две недели студенты изучат материал, на освоение которого в рамках традиционных программ уходит от пары месяцев до нескольких семестров. Они получат фундаментальные знания в области искусственного интеллекта, а также познакомятся с практиками применения нейросетей в сервисах Яндекса. В 2024 году пройдут четыре очных студкемпа, участвовать в которых могут студенты вузов из всех регионов России.\n",
    "\n",
    "Каждый студкемп посвящен одной из областей компьютерных наук: разработке ПО, машинному обучению, науке о данных и искусственному интеллекту. Авторы и преподаватели — эксперты Яндекса и Школы анализа данных, а также исследователи и преподаватели ведущих российских вузов. Обучение проходит на площадках партнёров: НИУ ВШЭ, Университет ИТМО, Университет Иннополис и УрФУ.\n",
    "\n",
    "С 1 по 13 апреля в Москве пройдет студкемп по машинному обучению на базе факультета компьютерных наук НИУ ВШЭ. Участники познакомятся с современными подходами в NLP и глубоком обучении, освоят методы сбора данных, в том числе с помощью YandexGPT, и научатся визуализировать результаты при помощи фреймворков. Во время итогового проекта студенты создадут MVP системы для решения задачи обработки естественного языка.\n",
    "\"\"\""
   ],
   "metadata": {
    "id": "Q0fy_RUAN7Ab"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Instansiate classes\n",
    "analyzer = Analyzer(nlp, russian_stopwords)\n",
    "vectorizer = Vectorizer(emb_model)\n",
    "clusterizer = Clusterizer()\n",
    "tagExtractor = FaissKeywordExtractor(emb_model)"
   ],
   "metadata": {
    "id": "N_0A6Zf4N7q7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Get tags\n",
    "nouns = analyzer.get_nouns(text1)\n",
    "vectorized_nouns = vectorizer.vectorize_nouns(nouns)\n",
    "cluster_centroids = clusterizer.get_clusters_centroids(vectorized_nouns)\n",
    "tags = tagExtractor.get_tags(cluster_centroids)\n",
    "tags"
   ],
   "metadata": {
    "id": "CCx2-mE4N-eB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Тестирование"
   ],
   "metadata": {
    "id": "74AKfgB3MR8W"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "from rouge import Rouge\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List\n",
    "\n",
    "# Downloading dataset\n",
    "data = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/new_data.csv\")\n",
    "X_test = data[\"text\"]\n",
    "y_test = data[\"tag\"].apply(ast.literal_eval).to_list()"
   ],
   "metadata": {
    "id": "u8Nhypkq-EZR"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Defining metric\n",
    "def rogue_score_corpus(true: List[List[str]], pred: List[List[str]]) -> Dict[str, float]:\n",
    "    \"\"\"Calculate ROGUE-1 (precision, recall and f1-score)\n",
    "\n",
    "    `Recall = num_words_matches / num_words_in_reference`\n",
    "\n",
    "    `Precision = num_words_matches / num_words_in_summary`\n",
    "\n",
    "    `F1 = classic f1 score`\n",
    "\n",
    "    Args:\n",
    "        true (List[List[str]]): True list with lists of tags for texts\n",
    "        pred (List[List[str]]): Predicted tags for texts\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: {\"recall\": num, \"precision\": num, \"f1\": num}\n",
    "    \"\"\"\n",
    "\n",
    "    rouge = Rouge()\n",
    "    rec, prec, f1 = list(), list(), list()\n",
    "\n",
    "    for true_tags, pred_tags in tqdm(zip(true, pred)):\n",
    "        if len(true_tags) == 0 or len(pred_tags) == 0:\n",
    "            continue\n",
    "\n",
    "        true_tags_str = \" \".join(true_tags)\n",
    "        pred_tags_str = \" \".join(pred_tags)\n",
    "\n",
    "        scores = rouge.get_scores(pred_tags_str, true_tags_str)\n",
    "        rogue_1 = scores[0][\"rouge-1\"]\n",
    "\n",
    "        rec.append(rogue_1[\"r\"])\n",
    "        prec.append(rogue_1[\"p\"])\n",
    "        f1.append(rogue_1[\"f\"])\n",
    "\n",
    "    return {\n",
    "        \"recall\": np.mean(rec),\n",
    "        \"precision\": np.mean(prec),\n",
    "        \"f1\": np.mean(f1),\n",
    "    }"
   ],
   "metadata": {
    "id": "qAl9EuQe92AO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Instansiate classes\n",
    "analyzer = Analyzer(nlp, russian_stopwords)\n",
    "vectorizer = Vectorizer(emb_model)\n",
    "clusterizer = Clusterizer()\n",
    "tagExtractor = FaissKeywordExtractor(emb_model)"
   ],
   "metadata": {
    "id": "4A2kvLRNOBWr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tags = []\n",
    "for text in tqdm(X_test):\n",
    "  nouns = analyzer.get_nouns(text)\n",
    "  vectorized_nouns = vectorizer.vectorize_nouns(nouns)\n",
    "  cluster_centroids = clusterizer.get_clusters_centroids(vectorized_nouns)\n",
    "  cur_tags = tagExtractor.get_tags(cluster_centroids)\n",
    "  tags.append(cur_tags)\n",
    "rogue_score_corpus(y_test, tags)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vObUm2Fp7s0K",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1712519634737,
     "user_tz": -180,
     "elapsed": 1165094,
     "user": {
      "displayName": "Денис Кажекин",
      "userId": "16634474198373539058"
     }
    },
    "outputId": "4797f5ef-69b4-4ed4-f410-aee8fe279900"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 3108/3108 [19:24<00:00,  2.67it/s]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "3108it [00:00, 11099.53it/s]\n",
    "{'recall': 0.17428784928784924,\n",
    " 'precision': 0.11730710302138872,\n",
    " 'f1': 0.1377398483478611}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
